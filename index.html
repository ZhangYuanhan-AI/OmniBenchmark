<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<link href="https://fonts.googleapis.com/css2?family=Open+Sans&display=swap"
      rel="stylesheet">
<link rel="stylesheet" type="text/css" href="./resources/style.css" media="screen"/>

<html lang="en">
<head>
  	<title>OmniBenchmark</title>
      <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/
          if you update and want to force Facebook to re-scrape. -->
  	<meta property="og:image" content="./resources/teaser-min.png"/>
  	<meta property="og:title" content="Benchmarking Omni-Vision Representation through the Lens of Visual Realms." />
  	<meta property="og:description" content="A new computer vision benchmark and a new super contrastive learning framework." />
    <!-- Twitter automatically scrapes this. Go to https://cards-dev.twitter.com/validator?
        if you update and want to force Twitter to re-scrape. -->
    <meta property="twitter:card"          content="summary" />
    <meta property="twitter:title"         content="Benchmarking Omni-Vision Representation through the Lens of Visual Realms." />
    <meta property="twitter:description"   content="A new computer vision benchmark and a new super contrastive learning framework." />
    <meta property="twitter:image"         content="./resources/teaser-min.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Add your Google Analytics tag here -->
    <!-- <script async
            src="https://www.googletagmanager.com/gtag/js?id=UA-97476543-1"></script> -->
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());
        gtag('config', 'UA-97476543-1');
    </script>

</head>

<body>
<div class="container">
    <div class="title">
        Benchmarking Omni-Vision Representation   <br> 
        through the Lens of Visual Realms
    </div>

    <div class="venue">
       In ECCV 2022
    </div>

    <br><br>

    <div class="author">
        <a href="https://zhangyuanhan-ai.github.io/">Yuanhan Zhang</a><sup>1</sup>
    </div>

    <div class="author">
        <a href="https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN">Zhenfei Yin</a><sup>2</sup>
    </div>

    <br>

    <div class="author">
        <a href="https://scholar.google.com.hk/citations?user=VU5ObUwAAAAJ&hl=zh-CN">Jing Shao</a><sup>2</sup>
    </div>
    <div class="author">
        <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1</sup>
    </div>

    <br><br>

    <div class="affiliation"><sup>1&nbsp;</sup>S-Lab, Nanyang Technological University</div>
    <div class="affiliation"><sup>2&nbsp;</sup>SenseTime Research</div>

    <br><br>

    
    <div class="links"><a href="">[Paper]</a></div>
    <!-- <div class="links"><a href="https://www.youtube.com/watch?v=dQw4w9WgXcQ">[Video]</a></div> -->
    <div class="links" ><a href="https://github.com/Davidzhangyuanhan/OmniBenchmark">[Code and Dataset]</a></div>
    <div class="links"><a href="https://zhangyuanhan-ai.github.io/OmniBenchmark/distribution/distribution.html">[Data Distribution]</a></div>
    <div class="links"><a href="https://paperswithcode.com/dataset/omnibenchmark">[Leaderboard]</a></div>
    <div class="links"><a href="https://github.com/Davidzhangyuanhan/OmniBenchmark">[Challenge]</a></div>

    <br><br>

    <img style="width: 80%;" src="./resources/teaser-min.png" alt="Teaser figure."/>
    <!-- <br>
    <p style="width: 80%;">
        This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful project</a>, and inherits the modifications made by <a href="https://github.com/jasonyzhang/webpage-template">Jason Zhang</a>.
        The code can be found <a href="https://github.com/elliottwu/webpage-template">here</a>.
    </p> -->

    <br><br>
    <hr>

    <h1>TL;DR</h1>
    <h2>New Benchmark</h2>
    <p style="width: 80%;">
        Omni-Realm Benchmark (OmniBenchmark) is a <strong>diverse</strong> (21 semantic realm-wise datasets) and <strong>concise</strong> (realm-wise datasets have no concepts overlapping) benchmark for evaluating pre-trained model generalization across semantic super-concepts/realms, e.g. across mammals to aircrafts.
    </p>
    <img id="gif-2" src="./resources/omnibenchmark.png" 
        onclick="document.getElementById('gif-2').src='./resources/Omnibenchmark_annimation.gif' " 
        onmouseout="document.getElementById('gif-2').src='./resources/omnibenchmark.png'" 
        style="width: 80%;"

    />

    <!-- <img style="width: 80%;" src="./resources/Omnibenchmark_annimation.gif" alt="omnibenchmark annimation."/> -->
    <br><br>
    <h2>New Supervised Contrastive Learning Framework</h2>
    <p style="width: 80%;">
        We introduces a new supervised contrastive learning framework, namely Relational Contrastive (ReCo) learning, that aims to be better suited for omni-vision representation.
    </p>
    <!-- <img style="width: 80%;" src="./resources/ReCo_annimation.gif" alt="reCo annimation."/> -->
    <img id="gif-3" src="./resources/reco.png" 
        onclick="document.getElementById('gif-3').src='./resources/ReCo_annimation.gif' " 
        onmouseout="document.getElementById('gif-3').src='./resources/reco.png'" 
        style="width: 80%;"

    />

    <br><br>
    <hr>


    <h1>Abstract</h1>
    <p style="width: 80%;">
        Though impressive performance has been achieved in specific visual realms (\eg faces, dogs, and places), an omni-vision representation generalizing to many natural visual domains is highly desirable.
        But, existing benchmarks are biased and inefficient to evaluate the omni-vision representation---these benchmarks either only include several specific realms, or cover most realms at the expense of subsuming numerous datasets that have extensive realm overlapping.
        In this paper, we propose Omni-Realm Benchmark (<strong>OmniBenchmark</strong>) It includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images. Without semantic overlapping, these datasets cover most visual realms comprehensively and meanwhile efficiently.
        In addition, we propose a new supervised contrastive learning framework, namely <strong>Re</strong>lational <strong>Co</strong>ntrastive learning (<strong>ReCo</strong>), for a better omni-vision representation.
        Beyond pulling two instances from the same concept closer---the typical supervised contrastive learning framework---ReCo also pulls two instances from the same semantic realm closer, encoding the semantic relation between concepts, facilitating omni-vision representation learning.
        We benchmark ReCo and other advances in omni-vision representation studies that are different in architectures (from CNNs to transformers) and in learning paradigms (from supervised learning to self-supervised learning) on OmniBenchmark.
        We illustrate the superior of ReCo to other supervised contrastive learning methods, and reveal multiple practical observations to facilitate future research.
    </p>

    <br><br>
    <hr>

    <!-- <h1>Video</h1>
    <div class="video-container">
        <iframe src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameBorder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
    </div>

    <br><br>
    <hr> -->

    <!-- <h1>Method Overview</h1>
    <img style="width: 80%;" src="./resources/method.jpg"
         alt="Method overview figure"/>
    <br>
    <a class="links" href="https://github.com/elliottwu/webpage-template">[Code]</a>

    <br><br>
    <hr> -->

    <!-- <h1>Results</h1>
    <img style="width: 80%;" src="./resources/results.jpg"
         alt="Results figure"/>

    <br><br>
    <hr> -->
    <h1>Paper</h1>
    <div class="paper-thumbnail">
        <a href="https://arxiv.org">
            <img class="layered-paper-big" width="100%" src="./resources/page1.png" alt="Paper thumbnail"/>
        </a>
    </div>
    <div class="paper-info">
        <h3>Benchmarking Omni-Vision Representation through the Lens of Visual Realms</h3>
        <p>Yuanhan Zhang, Zhenfei Yin, Jing Shao and Ziwei Liu</p>
        <p>In ECCV, 2022.</p>
        <pre><code>@InProceedings{zhang2022omnibenchmark,
            title = {Benchmarking Omni-Vision Representation through the Lens of Visual Realms},
            author = {Yuanhan Zhang, Zhenfei Yin, Jing Shao, Ziwei Liu},
            booktitle = {European Conference on Computer Vision (ECCV)},
            year = {2022},
        }</code></pre>
    </div>

    <br><br>
    <hr>

    <h1>Acknowledgements</h1>
    <p style="width: 80%;">
        This work is supported by NTU NAP, MOE AcRF Tier 2 (T2EP20221-0033), 
        and under the RIE2020 Industry Alignment Fund â€“ Industry Collaboration Projects (IAF-ICP) Funding Initiative, 
        as well as cash and in-kind contribution from the industry partner(s).
    </p>

    <br><br>
</div>

</body>

</html>
